# Library RAG - SystÃ¨me de Recherche Philosophique AvancÃ©

SystÃ¨me RAG (Retrieval-Augmented Generation) dual pour la recherche philosophique et la mÃ©moire conversationnelle, propulsÃ© par GPU embedder et Weaviate.

## ğŸ¯ Vue d'Ensemble

Library RAG combine deux systÃ¨mes de recherche sÃ©mantique distincts:

1. **ğŸ“š Library Philosophique** - Base documentaire de textes philosophiques (Å“uvres, chunks, rÃ©sumÃ©s)
2. **ğŸ§  Memory Ikario** - SystÃ¨me de mÃ©moire conversationnelle (pensÃ©es et conversations)

**Architecture**: 5 collections Weaviate + GPU embedder (NVIDIA RTX 4070) + Mistral API

## ğŸ—ï¸ Architecture

### Collections Weaviate (5)

```
ğŸ“¦ Library Philosophique (3 collections)
â”œâ”€ Work           â†’ MÃ©tadonnÃ©es des Å“uvres philosophiques
â”œâ”€ Chunk       â†’ 5355 passages de texte (1024-dim vectors)
â””â”€ Summary     â†’ RÃ©sumÃ©s hiÃ©rarchiques des documents

ğŸ§  Memory Ikario (2 collections)
â”œâ”€ Thought        â†’ 104 pensÃ©es (rÃ©flexions, insights)
â””â”€ Conversation   â†’ 12 conversations avec 380 messages
```

### GPU Embedder

- **ModÃ¨le**: BAAI/bge-m3 (1024 dimensions, 8192 tokens context)
- **GPU**: NVIDIA RTX 4070 Laptop (PyTorch CUDA + FP16)
- **Performance**: 30-70x plus rapide que Docker text2vec-transformers
- **Usage**: Vectorisation manuelle pour ingestion + requÃªtes

### Stack Technique

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **Vector DB** | Weaviate 1.34.4 | Stockage + recherche vectorielle |
| **Embeddings** | Python GPU embedder | Vectorisation (ingestion + requÃªtes) |
| **OCR** | Mistral OCR API | Extraction texte depuis PDF |
| **LLM** | Mistral Large / Ollama | GÃ©nÃ©ration de rÃ©ponses RAG |
| **Web** | Flask 3.0 + SSE | Interface web avec streaming |
| **Tests** | Puppeteer + pytest | Validation automatisÃ©e |

## ğŸš€ DÃ©marrage Rapide

### 1. PrÃ©requis

```bash
# Python 3.10+
python --version

# CUDA 12.4+ (pour GPU embedder)
nvidia-smi

# Docker (pour Weaviate)
docker --version
```

### 2. Installation

```bash
# Cloner le projet
git clone <repo-url>
cd linear_coding_library_rag

# CrÃ©er environnement virtuel
cd generations/library_rag
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Installer dÃ©pendances
pip install -r requirements.txt

# PyTorch avec CUDA (si pas dÃ©jÃ  installÃ©)
pip install torch --index-url https://download.pytorch.org/whl/cu124
```

### 3. Configuration

```bash
# Copier le fichier d'exemple
cp .env.example .env

# Ã‰diter .env avec vos clÃ©s API
nano .env
```

**Variables requises**:
```bash
# Mistral API (OCR + LLM)
MISTRAL_API_KEY=your-mistral-api-key

# Ollama (optionnel, pour LLM local)
OLLAMA_BASE_URL=http://localhost:11434
```

### 4. Lancer les Services

```bash
# DÃ©marrer Weaviate
docker compose up -d

# VÃ©rifier que Weaviate est prÃªt
curl http://localhost:8080/v1/.well-known/ready

# Lancer Flask
python flask_app.py
```

**URLs**:
- ğŸŒ Flask: http://localhost:5000
- ğŸ—„ï¸ Weaviate: http://localhost:8080

## ğŸ“– Utilisation

### Interface Web

AccÃ©der Ã  http://localhost:5000 pour:

| Page | URL | Description |
|------|-----|-------------|
| **Accueil** | `/` | Dashboard principal |
| **Recherche** | `/search` | Recherche dans library philosophique |
| **Chat** | `/chat` | Chat RAG avec contexte sÃ©mantique |
| **Memories** | `/memories` | Recherche dans pensÃ©es et messages |
| **Conversations** | `/conversations` | Historique des conversations |
| **Upload** | `/upload` | Ingestion de nouveaux PDF |

### 1. Recherche Philosophique

**Modes de recherche** (via `/search`):

- **ğŸ“„ Simple**: Recherche directe dans les chunks
- **ğŸŒ³ HiÃ©rarchique**: Recherche par sections avec contexte
- **ğŸ“š RÃ©sumÃ©s**: Recherche dans les rÃ©sumÃ©s de haut niveau

**Exemple**:
```
RequÃªte: "la conscience selon Turing"
â†’ 16 rÃ©sultats pertinents
â†’ Filtrage par auteur/Å“uvre
â†’ GPU embedder: ~17ms/requÃªte
```

### 2. Chat RAG

**FonctionnalitÃ©s** (via `/chat`):

- ğŸ’¬ RÃ©ponses longues et dÃ©taillÃ©es (500-800 mots)
- ğŸ“š Citations directes des passages sources
- ğŸ¯ Filtrage par Å“uvres (18 Å“uvres disponibles)
- ğŸ”„ Streaming SSE (Server-Sent Events)
- ğŸ“– Section "Sources utilisÃ©es" obligatoire

**Exemple de session**:
```
Question: "What is a Turing machine?"
â†’ Recherche sÃ©mantique: 11 chunks sur 5 sections
â†’ GÃ©nÃ©ration LLM: ~30 secondes (Mistral Large)
â†’ RÃ©ponse acadÃ©mique dÃ©taillÃ©e avec sources
```

### 3. Memory Ikario

**Recherche dans pensÃ©es** (via `/memories`):

```
RequÃªte: "test search"
â†’ 10 pensÃ©es pertinentes
â†’ Type: reflection, test, spontaneous
â†’ Concepts associÃ©s
```

**Recherche dans conversations**:

```
RequÃªte: "philosophie intelligence"
â†’ Conversations pertinentes
â†’ Messages contextuels
â†’ MÃ©tadonnÃ©es (catÃ©gorie, date)
```

### 4. Ingestion de Documents

**Via interface web** (`/upload`):

1. Upload PDF (max 100 MB)
2. SÃ©lection options:
   - LLM provider (Mistral/Ollama)
   - Chunking sÃ©mantique (optionnel)
   - OCR annotations (optionnel)
3. Traitement automatique:
   - OCR Mistral (~0.003â‚¬/page)
   - Extraction mÃ©tadonnÃ©es (auteur, titre, annÃ©e)
   - Chunking intelligent
   - Vectorisation GPU (~15ms/chunk)
   - Insertion Weaviate

**Via Python**:

```python
from utils.pdf_pipeline import process_pdf

result = process_pdf(
    pdf_path="document.pdf",
    use_llm=True,
    llm_provider="mistral",
    ingest_to_weaviate=True
)

print(f"Chunks: {result['chunks_count']}")
print(f"Cost: â‚¬{result['cost_total']:.4f}")
```

## ğŸ§ª Tests

### Tests AutomatisÃ©s

```bash
# Test ingestion GPU
python test_gpu_mistral.py

# Test recherche sÃ©mantique (Puppeteer)
node test_search_simple.js

# Test chat RAG (Puppeteer)
node test_chat_puppeteer.js

# Test memories/conversations (Puppeteer)
node test_memories_conversations.js
```

**RÃ©sultats attendus**:
- âœ… Ingestion: 9 chunks en ~1.2s
- âœ… Recherche: 16 rÃ©sultats en ~2s
- âœ… Chat: 11 chunks, 5 sections, rÃ©ponse complÃ¨te
- âœ… Memories: API backend fonctionnelle

### Tests Manuels

```bash
# VÃ©rifier GPU embedder
curl http://localhost:5000/search?q=Turing

# VÃ©rifier Weaviate
curl http://localhost:8080/v1/meta

# VÃ©rifier nombre de chunks
python -c "import weaviate; c=weaviate.connect_to_local(); print(c.collections.get('Chunk').aggregate.over_all()); c.close()"
```

## ğŸ“Š MÃ©triques de Performance

### Ingestion

| MÃ©trique | Avant (Docker) | AprÃ¨s (GPU) | AmÃ©lioration |
|----------|---------------|-------------|--------------|
| **Vitesse** | 500-1000ms/chunk | 15ms/chunk | **30-70x** |
| **RAM** | 10 GB (container) | 0 GB | **-10 GB** |
| **VRAM** | 0 GB | 2.6 GB | +2.6 GB |
| **Architecture** | Hybride | UnifiÃ©e | SimplifiÃ©e |

### Recherche

| OpÃ©ration | Temps | DÃ©tails |
|-----------|-------|---------|
| **Vectorisation requÃªte** | ~17ms | GPU embedder (modÃ¨le chargÃ©) |
| **Recherche Weaviate** | ~100-500ms | Selon complexitÃ© |
| **Recherche hiÃ©rarchique** | ~500ms | 11 chunks sur 5 sections |
| **Chat complet** | ~30s | Inclut gÃ©nÃ©ration LLM |

### Ressources

- **VRAM**: 2.6 GB peak (RTX 4070, 8 GB disponibles)
- **ModÃ¨le**: BAAI/bge-m3 (1024 dims, FP16 precision)
- **Batch size**: 48 (optimal pour RTX 4070)

## ğŸ”§ Configuration AvancÃ©e

### GPU Embedder

**Fichier**: `memory/core/embedding_service.py`

```python
class GPUEmbeddingService:
    model_name = "BAAI/bge-m3"
    embedding_dim = 1024
    optimal_batch_size = 48  # Ajuster selon GPU
```

**RÃ©duire VRAM** (si Out of Memory):
```python
optimal_batch_size = 24  # Au lieu de 48
```

### Weaviate

**Fichier**: `docker-compose.yml`

```yaml
services:
  weaviate:
    mem_limit: 8g        # Limiter RAM
    cpus: 4              # Limiter CPU
```

### LLM Chat

**Fichier**: `flask_app.py` (ligne 1272)

```python
# Personnaliser le prompt systÃ¨me
system_instruction = """
Vous Ãªtes un assistant expert en philosophie...
"""
```

## ğŸ“š Documentation

### Structure du Projet

```
generations/library_rag/
â”œâ”€â”€ flask_app.py              # Application Flask principale
â”œâ”€â”€ schema.py                 # SchÃ©mas Weaviate (5 collections)
â”œâ”€â”€ docker-compose.yml        # Weaviate (sans text2vec-transformers)
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ .env.example              # Configuration exemple
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pdf_pipeline.py       # Pipeline ingestion PDF
â”‚   â”œâ”€â”€ weaviate_ingest.py    # Ingestion GPU vectorization
â”‚   â”œâ”€â”€ llm_metadata.py       # Extraction mÃ©tadonnÃ©es LLM
â”‚   â””â”€â”€ ocr_processor.py      # Mistral OCR
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ embedding_service.py  # GPU embedder
â”œâ”€â”€ templates/                # Templates HTML
â””â”€â”€ static/                   # CSS, JS, images

docs/
â”œâ”€â”€ migration-gpu/            # Documentation migration GPU embedder
â”‚   â”œâ”€â”€ MIGRATION_GPU_EMBEDDER_SUCCESS.md
â”‚   â”œâ”€â”€ TESTS_COMPLETS_GPU_EMBEDDER.md
â”‚   â””â”€â”€ ...
â””â”€â”€ project_progress.md       # Historique dÃ©veloppement

tests/
â”œâ”€â”€ test_gpu_mistral.py       # Test ingestion
â”œâ”€â”€ test_search_simple.js     # Test recherche
â”œâ”€â”€ test_chat_puppeteer.js    # Test chat
â””â”€â”€ test_memories_conversations.js  # Test memories
```

### Documentation DÃ©taillÃ©e

- **[Migration GPU Embedder](docs/migration-gpu/MIGRATION_GPU_EMBEDDER_SUCCESS.md)** - Rapport de migration dÃ©taillÃ©
- **[Tests Complets](docs/migration-gpu/TESTS_COMPLETS_GPU_EMBEDDER.md)** - RÃ©sultats de tous les tests
- **[Project Progress](docs/project_progress.md)** - Historique du dÃ©veloppement
- **[CHANGELOG](CHANGELOG.md)** - Historique des versions

## ğŸ› DÃ©pannage

### ProblÃ¨me: "No module named 'memory'"

**Solution**:
```python
# VÃ©rifier sys.path dans weaviate_ingest.py
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
```

### ProblÃ¨me: "CUDA not available"

**Solution**:
```bash
# RÃ©installer PyTorch avec CUDA
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu124
```

### ProblÃ¨me: "Out of Memory (VRAM)"

**Solution**:
```python
# RÃ©duire batch size dans embedding_service.py
optimal_batch_size = 24  # Au lieu de 48
```

### ProblÃ¨me: Weaviate connection failed

**Solution**:
```bash
# VÃ©rifier que Weaviate est lancÃ©
docker compose ps

# VÃ©rifier les logs
docker compose logs weaviate

# RedÃ©marrer si nÃ©cessaire
docker compose restart
```

### ProblÃ¨me: Recherche ne renvoie rien

**Solution**:
```bash
# VÃ©rifier nombre de chunks dans Weaviate
python -c "import weaviate; c=weaviate.connect_to_local(); print(f'Chunks: {c.collections.get(\"Chunk\").aggregate.over_all().total_count}'); c.close()"

# RÃ©injecter les donnÃ©es si nÃ©cessaire
python schema.py --recreate-chunk
```

## ğŸ” SÃ©curitÃ©

- `.env` dans `.gitignore` (ne jamais commit les clÃ©s API)
- API Mistral: Facturation par usage (~â‚¬0.003/page OCR)
- Weaviate: Pas d'authentification (dev local uniquement)
- Flask: Mode debug (dÃ©sactiver en production)

## ğŸ“ˆ Roadmap

### Court Terme
- [ ] Monitorer performance GPU en production
- [ ] Benchmarks formels sur gros documents (100+ pages)
- [ ] Tests unitaires pour `vectorize_chunks_batch()`

### Moyen Terme
- [ ] API REST complÃ¨te (OpenAPI/Swagger)
- [ ] Support multi-utilisateurs avec authentification
- [ ] Export rÃ©sultats (PDF, Word, citations)

### Long Terme
- [ ] Fine-tuning BGE-M3 sur corpus philosophique
- [ ] Support langues supplÃ©mentaires (grec ancien, latin)
- [ ] Clustering automatique des concepts philosophiques

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amazing`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

MIT License - voir [LICENSE](LICENSE) pour dÃ©tails.

## ğŸ™ Remerciements

- **Weaviate** - Vector database
- **BAAI** - BGE-M3 embedding model
- **Mistral AI** - OCR et LLM API
- **Anthropic** - Claude pour dÃ©veloppement assistÃ©

---

**GÃ©nÃ©rÃ© avec**: Claude Sonnet 4.5
**DerniÃ¨re mise Ã  jour**: Janvier 2026
**Version**: 2.0 (GPU Embedder Migration)
