#!/usr/bin/env python3
"""V√©rification de la qualit√© des donn√©es Weaviate ≈ìuvre par ≈ìuvre.

Ce script analyse la coh√©rence entre les 4 collections (Work, Document, Chunk, Summary)
et d√©tecte les incoh√©rences :
- Documents sans chunks/summaries
- Chunks/summaries orphelins
- Works manquants
- Incoh√©rences dans les nested objects

Usage:
    python verify_data_quality.py
"""

import sys
from typing import Any, Dict, List, Set, Optional
from collections import defaultdict

import weaviate
from weaviate.collections import Collection


# =============================================================================
# Data Quality Checks
# =============================================================================


class DataQualityReport:
    """Rapport de qualit√© des donn√©es."""

    def __init__(self) -> None:
        self.total_documents = 0
        self.total_chunks = 0
        self.total_summaries = 0
        self.total_works = 0

        self.documents: List[Dict[str, Any]] = []
        self.issues: List[str] = []
        self.warnings: List[str] = []

        # Tracking des ≈ìuvres uniques extraites des nested objects
        self.unique_works: Dict[str, Set[str]] = defaultdict(set)  # title -> set(authors)

    def add_issue(self, severity: str, message: str) -> None:
        """Ajouter un probl√®me d√©tect√©."""
        if severity == "ERROR":
            self.issues.append(f"‚ùå {message}")
        elif severity == "WARNING":
            self.warnings.append(f"‚ö†Ô∏è  {message}")

    def add_document(self, doc_data: Dict[str, Any]) -> None:
        """Ajouter les donn√©es d'un document analys√©."""
        self.documents.append(doc_data)

    def print_report(self) -> None:
        """Afficher le rapport complet."""
        print("\n" + "=" * 80)
        print("RAPPORT DE QUALIT√â DES DONN√âES WEAVIATE")
        print("=" * 80)

        # Statistiques globales
        print("\nüìä STATISTIQUES GLOBALES")
        print("‚îÄ" * 80)
        print(f"  ‚Ä¢ Works (collection) :     {self.total_works:>6,} objets")
        print(f"  ‚Ä¢ Documents :              {self.total_documents:>6,} objets")
        print(f"  ‚Ä¢ Chunks :                 {self.total_chunks:>6,} objets")
        print(f"  ‚Ä¢ Summaries :              {self.total_summaries:>6,} objets")
        print()
        print(f"  ‚Ä¢ ≈íuvres uniques (nested): {len(self.unique_works):>6,} d√©tect√©es")

        # ≈íuvres uniques d√©tect√©es dans nested objects
        if self.unique_works:
            print("\nüìö ≈íUVRES D√âTECT√âES (via nested objects dans Chunks)")
            print("‚îÄ" * 80)
            for i, (title, authors) in enumerate(sorted(self.unique_works.items()), 1):
                authors_str = ", ".join(sorted(authors))
                print(f"  {i:2d}. {title}")
                print(f"      Auteur(s): {authors_str}")

        # Analyse par document
        print("\n" + "=" * 80)
        print("ANALYSE D√âTAILL√âE PAR DOCUMENT")
        print("=" * 80)

        for i, doc in enumerate(self.documents, 1):
            status = "‚úÖ" if doc["chunks_count"] > 0 and doc["summaries_count"] > 0 else "‚ö†Ô∏è"
            print(f"\n{status} [{i}/{len(self.documents)}] {doc['sourceId']}")
            print("‚îÄ" * 80)

            # M√©tadonn√©es Document
            if doc.get("work_nested"):
                work = doc["work_nested"]
                print(f"  ≈íuvre :     {work.get('title', 'N/A')}")
                print(f"  Auteur :    {work.get('author', 'N/A')}")
            else:
                print(f"  ≈íuvre :     {doc.get('title', 'N/A')}")
                print(f"  Auteur :    {doc.get('author', 'N/A')}")

            print(f"  √âdition :   {doc.get('edition', 'N/A')}")
            print(f"  Langue :    {doc.get('language', 'N/A')}")
            print(f"  Pages :     {doc.get('pages', 0):,}")

            # Collections
            print()
            print(f"  üì¶ Collections :")
            print(f"     ‚Ä¢ Chunks :    {doc['chunks_count']:>6,} objets")
            print(f"     ‚Ä¢ Summaries : {doc['summaries_count']:>6,} objets")

            # Work collection
            if doc.get("has_work_object"):
                print(f"     ‚Ä¢ Work :      ‚úÖ Existe dans collection Work")
            else:
                print(f"     ‚Ä¢ Work :      ‚ùå MANQUANT dans collection Work")

            # Coh√©rence nested objects
            if doc.get("nested_works_consistency"):
                consistency = doc["nested_works_consistency"]
                if consistency["is_consistent"]:
                    print(f"     ‚Ä¢ Coh√©rence nested objects : ‚úÖ OK")
                else:
                    print(f"     ‚Ä¢ Coh√©rence nested objects : ‚ö†Ô∏è  INCOH√âRENCES D√âTECT√âES")
                    if consistency["unique_titles"] > 1:
                        print(f"         ‚Üí {consistency['unique_titles']} titres diff√©rents dans chunks:")
                        for title in consistency["titles"]:
                            print(f"            - {title}")
                    if consistency["unique_authors"] > 1:
                        print(f"         ‚Üí {consistency['unique_authors']} auteurs diff√©rents dans chunks:")
                        for author in consistency["authors"]:
                            print(f"            - {author}")

            # Ratios
            if doc["chunks_count"] > 0:
                ratio = doc["summaries_count"] / doc["chunks_count"]
                print(f"  üìä Ratio Summary/Chunk : {ratio:.2f}")

                if ratio < 0.5:
                    print(f"     ‚ö†Ô∏è  Ratio faible (< 0.5) - Peut-√™tre des summaries manquants")
                elif ratio > 3.0:
                    print(f"     ‚ö†Ô∏è  Ratio √©lev√© (> 3.0) - Beaucoup de summaries pour peu de chunks")

            # Probl√®mes sp√©cifiques √† ce document
            if doc.get("issues"):
                print(f"\n  ‚ö†Ô∏è  Probl√®mes d√©tect√©s :")
                for issue in doc["issues"]:
                    print(f"     ‚Ä¢ {issue}")

        # Probl√®mes globaux
        if self.issues or self.warnings:
            print("\n" + "=" * 80)
            print("PROBL√àMES D√âTECT√âS")
            print("=" * 80)

            if self.issues:
                print("\n‚ùå ERREURS CRITIQUES :")
                for issue in self.issues:
                    print(f"  {issue}")

            if self.warnings:
                print("\n‚ö†Ô∏è  AVERTISSEMENTS :")
                for warning in self.warnings:
                    print(f"  {warning}")

        # Recommandations
        print("\n" + "=" * 80)
        print("RECOMMANDATIONS")
        print("=" * 80)

        if self.total_works == 0 and len(self.unique_works) > 0:
            print("\nüìå Collection Work vide")
            print(f"   ‚Ä¢ {len(self.unique_works)} ≈ìuvres uniques d√©tect√©es dans nested objects")
            print(f"   ‚Ä¢ Recommandation : Peupler la collection Work")
            print(f"   ‚Ä¢ Commande : python migrate_add_work_collection.py")
            print(f"   ‚Ä¢ Ensuite : Cr√©er des objets Work depuis les nested objects uniques")

        # V√©rifier coh√©rence counts
        total_chunks_declared = sum(doc.get("chunksCount", 0) for doc in self.documents if "chunksCount" in doc)
        if total_chunks_declared != self.total_chunks:
            print(f"\n‚ö†Ô∏è  Incoh√©rence counts")
            print(f"   ‚Ä¢ Document.chunksCount total : {total_chunks_declared:,}")
            print(f"   ‚Ä¢ Chunks r√©els :                {self.total_chunks:,}")
            print(f"   ‚Ä¢ Diff√©rence :                  {abs(total_chunks_declared - self.total_chunks):,}")

        print("\n" + "=" * 80)
        print("FIN DU RAPPORT")
        print("=" * 80)
        print()


def analyze_document_quality(
    all_chunks: List[Any],
    all_summaries: List[Any],
    doc_sourceId: str,
    client: weaviate.WeaviateClient,
) -> Dict[str, Any]:
    """Analyser la qualit√© des donn√©es pour un document sp√©cifique.

    Args:
        all_chunks: All chunks from database (to filter in Python).
        all_summaries: All summaries from database (to filter in Python).
        doc_sourceId: Document identifier to analyze.
        client: Connected Weaviate client.

    Returns:
        Dict containing analysis results.
    """
    result: Dict[str, Any] = {
        "sourceId": doc_sourceId,
        "chunks_count": 0,
        "summaries_count": 0,
        "has_work_object": False,
        "issues": [],
    }

    # Filtrer les chunks associ√©s (en Python car nested objects non filtrables)
    try:
        doc_chunks = [
            chunk for chunk in all_chunks
            if chunk.properties.get("document", {}).get("sourceId") == doc_sourceId
        ]

        result["chunks_count"] = len(doc_chunks)

        # Analyser coh√©rence nested objects
        if doc_chunks:
            titles: Set[str] = set()
            authors: Set[str] = set()

            for chunk_obj in doc_chunks:
                props = chunk_obj.properties
                if "work" in props and isinstance(props["work"], dict):
                    work = props["work"]
                    if work.get("title"):
                        titles.add(work["title"])
                    if work.get("author"):
                        authors.add(work["author"])

            result["nested_works_consistency"] = {
                "titles": sorted(titles),
                "authors": sorted(authors),
                "unique_titles": len(titles),
                "unique_authors": len(authors),
                "is_consistent": len(titles) <= 1 and len(authors) <= 1,
            }

            # R√©cup√©rer work/author pour ce document
            if titles and authors:
                result["work_from_chunks"] = {
                    "title": list(titles)[0] if len(titles) == 1 else titles,
                    "author": list(authors)[0] if len(authors) == 1 else authors,
                }

    except Exception as e:
        result["issues"].append(f"Erreur analyse chunks: {e}")

    # Filtrer les summaries associ√©s (en Python)
    try:
        doc_summaries = [
            summary for summary in all_summaries
            if summary.properties.get("document", {}).get("sourceId") == doc_sourceId
        ]

        result["summaries_count"] = len(doc_summaries)

    except Exception as e:
        result["issues"].append(f"Erreur analyse summaries: {e}")

    # V√©rifier si Work existe
    if result.get("work_from_chunks"):
        work_info = result["work_from_chunks"]
        if isinstance(work_info["title"], str):
            try:
                work_collection = client.collections.get("Work")
                work_response = work_collection.query.fetch_objects(
                    filters=weaviate.classes.query.Filter.by_property("title").equal(work_info["title"]),
                    limit=1,
                )

                result["has_work_object"] = len(work_response.objects) > 0

            except Exception as e:
                result["issues"].append(f"Erreur v√©rification Work: {e}")

    # D√©tection de probl√®mes
    if result["chunks_count"] == 0:
        result["issues"].append("Aucun chunk trouv√© pour ce document")

    if result["summaries_count"] == 0:
        result["issues"].append("Aucun summary trouv√© pour ce document")

    if result.get("nested_works_consistency") and not result["nested_works_consistency"]["is_consistent"]:
        result["issues"].append("Incoh√©rences dans les nested objects work")

    return result


def main() -> None:
    """Main entry point."""
    # Fix encoding for Windows console
    if sys.platform == "win32" and hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')

    print("=" * 80)
    print("V√âRIFICATION DE LA QUALIT√â DES DONN√âES WEAVIATE")
    print("=" * 80)
    print()

    client = weaviate.connect_to_local(
        host="localhost",
        port=8080,
        grpc_port=50051,
    )

    try:
        if not client.is_ready():
            print("‚ùå Weaviate is not ready. Ensure docker-compose is running.")
            sys.exit(1)

        print("‚úì Weaviate is ready")
        print("‚úì Starting data quality analysis...")
        print()

        report = DataQualityReport()

        # R√©cup√©rer counts globaux
        try:
            work_coll = client.collections.get("Work")
            work_result = work_coll.aggregate.over_all(total_count=True)
            report.total_works = work_result.total_count
        except Exception as e:
            report.add_issue("ERROR", f"Cannot count Work objects: {e}")

        try:
            chunk_coll = client.collections.get("Chunk")
            chunk_result = chunk_coll.aggregate.over_all(total_count=True)
            report.total_chunks = chunk_result.total_count
        except Exception as e:
            report.add_issue("ERROR", f"Cannot count Chunk objects: {e}")

        try:
            summary_coll = client.collections.get("Summary")
            summary_result = summary_coll.aggregate.over_all(total_count=True)
            report.total_summaries = summary_result.total_count
        except Exception as e:
            report.add_issue("ERROR", f"Cannot count Summary objects: {e}")

        # R√©cup√©rer TOUS les chunks et summaries en une fois
        # (car nested objects non filtrables via API Weaviate)
        print("Loading all chunks and summaries into memory...")
        all_chunks: List[Any] = []
        all_summaries: List[Any] = []

        try:
            chunk_coll = client.collections.get("Chunk")
            chunks_response = chunk_coll.query.fetch_objects(
                limit=10000,  # Haute limite pour gros corpus
                # Note: nested objects (work, document) sont retourn√©s automatiquement
            )
            all_chunks = chunks_response.objects
            print(f"  ‚úì Loaded {len(all_chunks)} chunks")
        except Exception as e:
            report.add_issue("ERROR", f"Cannot fetch all chunks: {e}")

        try:
            summary_coll = client.collections.get("Summary")
            summaries_response = summary_coll.query.fetch_objects(
                limit=10000,
                # Note: nested objects (document) sont retourn√©s automatiquement
            )
            all_summaries = summaries_response.objects
            print(f"  ‚úì Loaded {len(all_summaries)} summaries")
        except Exception as e:
            report.add_issue("ERROR", f"Cannot fetch all summaries: {e}")

        print()

        # R√©cup√©rer tous les documents
        try:
            doc_collection = client.collections.get("Document")
            docs_response = doc_collection.query.fetch_objects(
                limit=1000,
                return_properties=["sourceId", "title", "author", "edition", "language", "pages", "chunksCount", "work"],
            )

            report.total_documents = len(docs_response.objects)

            print(f"Analyzing {report.total_documents} documents...")
            print()

            for doc_obj in docs_response.objects:
                props = doc_obj.properties
                doc_sourceId = props.get("sourceId", "unknown")

                print(f"  ‚Ä¢ Analyzing {doc_sourceId}...", end=" ")

                # Analyser ce document (avec filtrage Python)
                analysis = analyze_document_quality(all_chunks, all_summaries, doc_sourceId, client)

                # Merger props Document avec analysis
                analysis.update({
                    "title": props.get("title"),
                    "author": props.get("author"),
                    "edition": props.get("edition"),
                    "language": props.get("language"),
                    "pages": props.get("pages", 0),
                    "chunksCount": props.get("chunksCount", 0),
                    "work_nested": props.get("work"),
                })

                # Collecter ≈ìuvres uniques
                if analysis.get("work_from_chunks"):
                    work_info = analysis["work_from_chunks"]
                    if isinstance(work_info["title"], str) and isinstance(work_info["author"], str):
                        report.unique_works[work_info["title"]].add(work_info["author"])

                report.add_document(analysis)

                # Feedback
                if analysis["chunks_count"] > 0:
                    print(f"‚úì ({analysis['chunks_count']} chunks, {analysis['summaries_count']} summaries)")
                else:
                    print("‚ö†Ô∏è  (no chunks)")

        except Exception as e:
            report.add_issue("ERROR", f"Cannot fetch documents: {e}")

        # V√©rifications globales
        if report.total_works == 0 and report.total_chunks > 0:
            report.add_issue("WARNING", f"Work collection is empty but {report.total_chunks:,} chunks exist")

        if report.total_documents == 0 and report.total_chunks > 0:
            report.add_issue("WARNING", f"No documents but {report.total_chunks:,} chunks exist (orphan chunks)")

        # Afficher le rapport
        report.print_report()

    finally:
        client.close()


if __name__ == "__main__":
    main()
