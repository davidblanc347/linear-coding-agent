# Library RAG - Base de Textes Philosophiques

SystÃ¨me RAG (Retrieval Augmented Generation) de qualitÃ© production spÃ©cialisÃ© dans l'indexation et la recherche sÃ©mantique de textes philosophiques et acadÃ©miques. Pipeline complet d'OCR, extraction de mÃ©tadonnÃ©es, chunking intelligent et vectorisation automatique.

> **Note Technique (Dec 2024):** Migration vers BAAI/bge-m3 (1024-dim, 8192 token context) pour un support multilingue supÃ©rieur (grec, latin, franÃ§ais, anglais) et des performances amÃ©liorÃ©es sur les textes philosophiques. Voir [Annexe: Migration BGE-M3](#annexe-migration-bge-m3).

---

## ğŸš€ DÃ©marrage Rapide

```bash
# 1. Configurer les variables d'environnement
cp .env.example .env
# Ã‰diter .env et ajouter votre MISTRAL_API_KEY

# 2. DÃ©marrer Weaviate + transformers
docker compose up -d

# 3. Installer les dÃ©pendances Python
pip install -r requirements.txt

# 4. CrÃ©er le schÃ©ma Weaviate
python schema.py

# 5. Lancer l'interface web Flask
python flask_app.py
```

Ouvrez ensuite http://localhost:5000 dans votre navigateur.

---

## ğŸ“– Table des MatiÃ¨res

- [Architecture](#-architecture)
- [Pipeline de Traitement PDF](#-pipeline-de-traitement-pdf-10-Ã©tapes)
- [Configuration](#%EF%B8%8F-configuration)
- [Interface Flask](#-interface-flask)
- [SchÃ©ma Weaviate](#-schÃ©ma-weaviate-6-collections)
- [Exemples de RequÃªtes](#-exemples-de-requÃªtes)
- [MCP Server (Claude Desktop)](#-mcp-server-claude-desktop)
- [Gestion des CoÃ»ts](#-gestion-des-coÃ»ts)
- [Tests](#-tests)
- [Debugging](#-debugging)
- [Production](#-production)
- [Annexes](#-annexes)

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TB
    subgraph Docker["ğŸ³ Docker Compose"]
        subgraph Weaviate["Weaviate 1.34.4 - 6 Collections"]
            direction TB
            subgraph RAG["ğŸ“š RAG Collections (3)"]
                Work["Work<br/><i>no vectorizer</i>"]
                Chunk["Chunk_v2<br/><i>GPU embedder</i>"]
                Summary["Summary_v2<br/><i>GPU embedder</i>"]
                Work --> Chunk
                Work --> Summary
            end

            subgraph Memory["ğŸ§  Memory Collections (3)"]
                Conv["Conversation<br/><i>GPU embedder</i>"]
                Msg["Message<br/><i>GPU embedder</i>"]
                Thought["Thought<br/><i>GPU embedder</i>"]
                Conv --> Msg
            end
        end
    end

    subgraph Flask["ğŸŒ Flask App"]
        Parser["ğŸ“„ Pipeline PDF<br/>10 Ã©tapes"]
        OCR["ğŸ” Mistral OCR"]
        LLM["ğŸ§  LLM<br/>Ollama / Mistral"]
        Web["ğŸ¨ Interface Web<br/>SSE Progress"]
    end

    GPUEmbed["âš¡ GPU Embedder<br/>BAAI/bge-m3 (RTX 4070)"]
    Client["ğŸ Python Client"]

    Client -->|"REST :8080<br/>gRPC :50051"| Weaviate
    Chunk -.->|manual vectors| GPUEmbed
    Summary -.->|manual vectors| GPUEmbed
    Conv -.->|manual vectors| GPUEmbed
    Msg -.->|manual vectors| GPUEmbed
    Thought -.->|manual vectors| GPUEmbed
    Parser --> OCR
    Parser --> LLM
    Parser --> Client
```

**Composants ClÃ©s:**
- **Weaviate 1.34.4**: Base vectorielle avec 6 collections (3 RAG + 3 Memory)
- **GPU Embedder**: Python BAAI/bge-m3 (1024-dim, RTX 4070, PyTorch CUDA)
- **Mistral OCR**: Extraction texte/images (~0.003â‚¬/page)
- **LLM**: Ollama (local, gratuit) ou Mistral API (rapide, payant)
- **Flask 3.0**: Interface web avec Server-Sent Events (SSE)

---

## ğŸ“„ Pipeline de Traitement PDF (10 Ã‰tapes)

Le systÃ¨me implÃ©mente un pipeline intelligent orchestrÃ© par `utils/pdf_pipeline.py` :

```mermaid
flowchart TD
    PDF["ğŸ“„ PDF Upload"] --> Step1["[1] OCR Mistral<br/>~0.003â‚¬/page"]
    Step1 --> Step2["[2] Markdown Builder<br/>Structure le texte"]
    Step2 --> Step3["[3] Image Extractor<br/>Sauvegarde images"]
    Step3 --> Step4["[4] LLM Metadata<br/>Titre, auteur, annÃ©e"]
    Step4 --> Step5["[5] LLM TOC<br/>Table des matiÃ¨res"]
    Step5 --> Step6["[6] LLM Classifier<br/>Classification sections"]
    Step6 --> Step7["[7] LLM Chunker<br/>Chunking sÃ©mantique"]
    Step7 --> Step8["[8] Cleaner<br/>Nettoyage OCR"]
    Step8 --> Step9["[9] LLM Validator<br/>Validation + concepts"]
    Step9 --> Step10["[10] Weaviate Ingest<br/>Vectorisation"]
    Step10 --> DB[("ğŸ—„ï¸ Weaviate<br/>6 Collections")]
```

### DÃ©tails du Pipeline

| Ã‰tape | Module | Fonction | CoÃ»t |
|-------|--------|----------|------|
| **1** | `ocr_processor.py` | Extraction texte/images via Mistral OCR | ~0.003â‚¬/page |
| **2** | `markdown_builder.py` | Construction Markdown structurÃ© | Gratuit |
| **3** | `image_extractor.py` | Sauvegarde images dans `output/images/` | Gratuit |
| **4** | `llm_metadata.py` | Extraction mÃ©tadonnÃ©es (titre, auteur, langue, annÃ©e) | Variable (LLM) |
| **5** | `llm_toc.py` | Extraction hiÃ©rarchique de la table des matiÃ¨res | Variable (LLM) |
| **6** | `llm_classifier.py` | Classification sections (main_content, preamble, etc.) | Variable (LLM) |
| **7** | `llm_chunker.py` | DÃ©coupage sÃ©mantique en unitÃ©s argumentatives | Variable (LLM) |
| **8** | `llm_cleaner.py` | Nettoyage artÃ©facts OCR, validation longueur | Gratuit |
| **9** | `llm_validator.py` | Validation chunks + extraction concepts/mots-clÃ©s | Variable (LLM) |
| **10** | `weaviate_ingest.py` | Ingestion batch + vectorisation automatique | Gratuit |

**Progression en Temps RÃ©el:** Server-Sent Events (SSE) pour suivre chaque Ã©tape du traitement via l'interface web.

---

## âš™ï¸ Configuration

### Variables d'Environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# API Mistral (obligatoire pour OCR)
MISTRAL_API_KEY=your_mistral_api_key_here

# LLM Configuration
STRUCTURE_LLM_MODEL=qwen2.5:7b             # ModÃ¨le Ollama (ou modÃ¨le Mistral)
OLLAMA_BASE_URL=http://localhost:11434    # URL serveur Ollama
STRUCTURE_LLM_TEMPERATURE=0.2             # TempÃ©rature LLM (0=dÃ©terministe, 1=crÃ©atif)

# APIs optionnelles (non utilisÃ©es actuellement)
ANTHROPIC_API_KEY=your_anthropic_key      # Optionnel
OPENAI_API_KEY=your_openai_key            # Optionnel

# Weaviate (defaults)
WEAVIATE_HOST=localhost
WEAVIATE_PORT=8080

# Linear Integration (pour dÃ©veloppement dans framework)
LINEAR_TEAM=LRP                           # Identifiant Ã©quipe Linear
```

### Options de Traitement

Lors de l'upload d'un PDF, vous pouvez configurer :

| Option | Par dÃ©faut | Description |
|--------|------------|-------------|
| `skip_ocr` | `False` | RÃ©utiliser markdown existant (Ã©vite coÃ»t OCR) |
| `use_llm` | `True` | Activer les Ã©tapes LLM (mÃ©tadonnÃ©es, TOC, chunking) |
| `llm_provider` | `"ollama"` | `"ollama"` (local, gratuit) ou `"mistral"` (API, rapide) |
| `llm_model` | `None` | Nom du modÃ¨le (auto-dÃ©tectÃ© depuis .env si None) |
| `use_ocr_annotations` | `False` | OCR avec annotations (3x coÃ»t, meilleure TOC) |
| `use_semantic_chunking` | `False` | Chunking LLM (lent mais prÃ©cis) |
| `ingest_to_weaviate` | `True` | InsÃ©rer les chunks dans Weaviate |

---

## ğŸ“Š SchÃ©ma Weaviate (6 Collections)

Le systÃ¨me utilise **6 collections Weaviate** organisÃ©es en **2 ensembles distincts et indÃ©pendants** :

**ğŸ¯ SÃ©paration des prÃ©occupations:**
- **Collections RAG** (`schema.py`) : DÃ©diÃ©es Ã  l'indexation et recherche de textes philosophiques
- **Collections Memory** (`memory/schemas/memory_schemas.py`) : DÃ©diÃ©es au systÃ¨me de mÃ©moire conversationnelle

Les deux ensembles partagent la mÃªme instance Weaviate et le mÃªme GPU embedder (BAAI/bge-m3) mais sont gÃ©rÃ©s via des modules sÃ©parÃ©s.

### ğŸ“š Collections RAG (3) - Textes Philosophiques

```
Work (no vectorizer)
  â”œâ”€ title, author, year, language, genre
  â”‚
  â”œâ”€â–º Chunk_v2 (VECTORIZED â­ - GPU embedder)
  â”‚     â”œâ”€ text (vectorized), keywords (vectorized)
  â”‚     â”œâ”€ workTitle, workAuthor, sectionPath, chapterTitle
  â”‚     â”œâ”€ unitType, orderIndex, language, year
  â”‚     â””â”€ work: {title, author} (nested)
  â”‚
  â””â”€â–º Summary_v2 (VECTORIZED â­ - GPU embedder)
        â”œâ”€ text (vectorized), concepts (vectorized)
        â”œâ”€ sectionPath, title, level, chunksCount
        â””â”€ work: {title, author} (nested)
```

### ğŸ§  Collections Memory (3) - SystÃ¨me de MÃ©moire

```
Conversation (VECTORIZED - GPU embedder)
  â”œâ”€ conversation_id, title, category
  â”œâ”€ summary (vectorized), tags
  â””â”€ timestamp, message_count

Message (VECTORIZED - GPU embedder)
  â”œâ”€ content (vectorized)
  â”œâ”€ role (user/assistant/system)
  â”œâ”€ conversation_id, order_index
  â””â”€ timestamp

Thought (VECTORIZED - GPU embedder)
  â”œâ”€ content (vectorized), concepts (vectorized)
  â”œâ”€ thought_type, trigger, emotional_state
  â””â”€ timestamp, privacy_level
```

### DÃ©tails des Collections RAG

**Work** (no vectorizer)
- ReprÃ©sente une Å“uvre philosophique (ex: MÃ©non de Platon)
- PropriÃ©tÃ©s : `title`, `author`, `originalTitle`, `year`, `language`, `genre`
- Pas de vectorisation (mÃ©tadonnÃ©es uniquement)
- **RÃ´le** : Source unique de vÃ©ritÃ© pour les mÃ©tadonnÃ©es des Å“uvres

**Chunk_v2 â­** (GPU embedder - BAAI/bge-m3, 1024-dim)
- Fragment de texte optimisÃ© pour la recherche sÃ©mantique (200-800 caractÃ¨res)
- PropriÃ©tÃ©s vectorisÃ©es : `text`, `keywords`
- PropriÃ©tÃ©s non-vectorisÃ©es : `workTitle`, `workAuthor`, `sectionPath`, `chapterTitle`, `unitType`, `orderIndex`, `language`, `year`
- RÃ©fÃ©rence nested : `work: {title, author}`
- **Vectorisation** : Manuelle avec Python GPU embedder (RTX 4070, PyTorch CUDA)

**Summary_v2 â­** (GPU embedder - BAAI/bge-m3, 1024-dim)
- RÃ©sumÃ©s LLM de chapitres/sections pour recherche de haut niveau
- PropriÃ©tÃ©s vectorisÃ©es : `text`, `concepts`
- PropriÃ©tÃ©s non-vectorisÃ©es : `sectionPath`, `title`, `level`, `chunksCount`
- RÃ©fÃ©rence nested : `work: {title, author}`
- **Vectorisation** : Manuelle avec Python GPU embedder

### DÃ©tails des Collections Memory

**Conversation** (GPU embedder)
- Conversations complÃ¨tes avec Claude Desktop
- PropriÃ©tÃ©s : `conversation_id`, `title`, `category`, `summary` (vectorized), `tags`, `timestamp`, `message_count`
- **Usage** : Recherche sÃ©mantique dans l'historique de conversations

**Message** (GPU embedder)
- Messages individuels dans les conversations
- PropriÃ©tÃ©s : `content` (vectorized), `role`, `conversation_id`, `order_index`, `timestamp`
- **Usage** : Recherche sÃ©mantique dans les messages spÃ©cifiques

**Thought** (GPU embedder)
- PensÃ©es/rÃ©flexions individuelles
- PropriÃ©tÃ©s : `content` (vectorized), `concepts` (vectorized), `thought_type`, `trigger`, `emotional_state`, `timestamp`, `privacy_level`
- **Usage** : SystÃ¨me de mÃ©moire pour insights et rÃ©flexions

### Design Patterns

**Nested Objects vs Cross-References:**
- Utilise des objets imbriquÃ©s pour Ã©viter les JOINs
- AccÃ¨s en une seule requÃªte avec mÃ©tadonnÃ©es Work/Document
- Trade-off : Petite duplication contrÃ´lÃ©e pour performance maximale

**Vectorisation SÃ©lective:**
- Seuls `Chunk.text/summary/keywords` et `Summary.text/concepts` sont vectorisÃ©s
- MÃ©tadonnÃ©es utilisent `skip_vectorization=True` pour filtrage rapide
- Gain : ~6Ã— moins de calculs vs vectorisation complÃ¨te

**Index Vectoriel HNSW + RQ (2026-01):**
- **HNSW** (Hierarchical Navigable Small World) : Index optimisÃ© pour recherche rapide
- **RQ** (Rotational Quantization) : Compression des vecteurs (~75% rÃ©duction mÃ©moire)
- **Distance** : Cosine similarity (compatible BGE-M3)
- **Performance** : <1% perte de prÃ©cision, scalable jusqu'Ã  100k+ chunks

---

## ğŸŒ Interface Flask

### Routes Disponibles

| Route | MÃ©thode | Description |
|-------|---------|-------------|
| `/` | GET | ğŸ›ï¸ Accueil â€” Statistiques des collections |
| `/passages` | GET | ğŸ“š Parcourir â€” Liste paginÃ©e de tous les chunks |
| `/search` | GET | ğŸ” Recherche â€” Recherche sÃ©mantique vectorielle |
| `/upload` | GET | ğŸ“¤ Formulaire â€” Page d'upload PDF |
| `/upload` | POST | ğŸš€ Traiter â€” DÃ©marre le traitement PDF en arriÃ¨re-plan |
| `/upload/progress/<job_id>` | GET | ğŸ“Š SSE â€” Flux de progression en temps rÃ©el |
| `/upload/status/<job_id>` | GET | â„¹ï¸ Statut â€” Ã‰tat JSON du job de traitement |
| `/upload/result/<job_id>` | GET | âœ… RÃ©sultats â€” Page de rÃ©sultats du traitement |
| `/documents` | GET | ğŸ“ Documents â€” Liste des documents traitÃ©s |
| `/documents/<doc>/view` | GET | ğŸ‘ï¸ DÃ©tails â€” Vue dÃ©taillÃ©e d'un document |
| `/documents/delete/<doc>` | POST | ğŸ—‘ï¸ Supprimer â€” Supprime document + chunks de Weaviate |
| `/output/<filepath>` | GET | ğŸ’¾ TÃ©lÃ©charger â€” TÃ©lÃ©charge fichiers traitÃ©s (MD, JSON) |

### Server-Sent Events (SSE)

L'interface utilise SSE pour un suivi en temps rÃ©el du traitement :

```javascript
// Exemple de flux SSE
event: step
data: {"step": 1, "total": 10, "message": "OCR Mistral en cours...", "progress": 10}

event: step
data: {"step": 4, "total": 10, "message": "Extraction mÃ©tadonnÃ©es (LLM)...", "progress": 40}

event: complete
data: {"success": true, "document": "platon-menon", "chunks": 127, "cost_ocr": 0.12, "cost_llm": 0.03}

event: error
data: {"error": "OCR failed: API timeout"}
```

---

## ğŸ” Exemples de RequÃªtes

### Recherche SÃ©mantique (Collection Chunk)

```python
import weaviate
import weaviate.classes.query as wvq

client = weaviate.connect_to_local()

try:
    chunks = client.collections.get("Chunk_v2")

    # Recherche sÃ©mantique simple
    result = chunks.query.near_text(
        query="la mort et la valeur de la vie",
        limit=5,
        return_metadata=wvq.MetadataQuery(distance=True),
    )

    for obj in result.objects:
        work = obj.properties['work']
        doc = obj.properties['document']
        print(f"[{work['title']} - {work['author']}]")
        print(f"  Edition: {doc['edition']}")
        print(f"  Section: {obj.properties['sectionPath']}")
        print(f"  {obj.properties['text'][:200]}...")
        print(f"  SimilaritÃ©: {(1 - obj.metadata.distance) * 100:.1f}%\n")

finally:
    client.close()
```

### Recherche avec Filtres

```python
# Rechercher dans les Å“uvres de Platon uniquement
result = chunks.query.near_text(
    query="justice et vÃ©ritÃ©",
    limit=10,
    filters=wvq.Filter.by_property("work").by_property("author").equal("Platon"),
    return_metadata=wvq.MetadataQuery(distance=True),
)

# Filtrer par langue
result = chunks.query.near_text(
    query="Ã¢me immortelle",
    limit=5,
    filters=wvq.Filter.by_property("language").equal("fr"),
)

# Filtrer par type d'unitÃ© (arguments uniquement)
result = chunks.query.near_text(
    query="connaissance",
    filters=wvq.Filter.by_property("unitType").equal("argument"),
)
```

### Recherche Hybride (SÃ©mantique + BM25)

```python
# Combine recherche vectorielle et recherche par mots-clÃ©s
result = chunks.query.hybrid(
    query="rÃ©miniscence et connaissance",
    alpha=0.75,  # 0 = BM25 uniquement, 1 = vectoriel uniquement, 0.75 = favorise vectoriel
    limit=10,
)
```

### Recherche dans les RÃ©sumÃ©s (High-Level)

```python
summaries = client.collections.get("Summary")

# Recherche de chapitres/sections par concept
result = summaries.query.near_text(
    query="dialectique et maÃ¯eutique",
    limit=5,
)

for obj in result.objects:
    print(f"Section: {obj.properties['title']}")
    print(f"Niveau: {obj.properties['level']}")
    print(f"RÃ©sumÃ©: {obj.properties['text']}")
    print(f"Concepts: {', '.join(obj.properties['concepts'])}\n")
```

---

## ğŸ¤– MCP Server (Claude Desktop)

Library RAG expose ses fonctionnalitÃ©s via un serveur MCP (Model Context Protocol) pour intÃ©gration avec Claude Desktop.

### Installation MCP

```bash
# Installer les dÃ©pendances MCP
pip install -r requirements.txt

# Tester le serveur
python mcp_server.py
```

### Configuration Claude Desktop

Ajouter Ã  votre configuration Claude Desktop :

**Windows:** `%APPDATA%\Claude\claude_desktop_config.json`
**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`
**Linux:** `~/.config/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "library-rag": {
      "command": "python",
      "args": ["C:/path/to/library_rag/mcp_server.py"],
      "env": {
        "MISTRAL_API_KEY": "your-mistral-api-key"
      }
    }
  }
}
```

### Outils MCP Disponibles

**1. parse_pdf** - Traite un PDF avec paramÃ¨tres optimaux
```
parse_pdf(pdf_path="/docs/platon-menon.pdf")
```

**2. search_chunks** - Recherche sÃ©mantique dans les chunks
```
search_chunks(query="la vertu", limit=10, author_filter="Platon")
```

**3. search_summaries** - Recherche dans les rÃ©sumÃ©s de chapitres
```
search_summaries(query="dialectique", min_level=1, max_level=2)
```

**4. get_document** - RÃ©cupÃ¨re un document par ID
```
get_document(source_id="platon-menon", include_chunks=true)
```

**5. list_documents** - Liste tous les documents
```
list_documents(author_filter="Platon", language_filter="fr")
```

**6. get_chunks_by_document** - RÃ©cupÃ¨re les chunks d'un document
```
get_chunks_by_document(source_id="platon-menon", limit=50)
```

**7. filter_by_author** - Tous les travaux d'un auteur
```
filter_by_author(author="Platon")
```

**8. delete_document** - Supprime un document (requiert confirmation)
```
delete_document(source_id="platon-menon", confirm=true)
```

### Outils MCP Memory (9 outils intÃ©grÃ©s - Phase 4)

**SystÃ¨me de MÃ©moire UnifiÃ©** : Le serveur MCP intÃ¨gre dÃ©sormais 9 outils pour gÃ©rer un systÃ¨me de mÃ©moire (Thoughts, Messages, Conversations) utilisant Weaviate + GPU embeddings. Ces outils permettent Ã  Claude Desktop de crÃ©er, rechercher et gÃ©rer des pensÃ©es, messages et conversations de maniÃ¨re persistante.

**Architecture Memory** :
- **Backend** : Weaviate 1.34.4 (collections Thought, Message, Conversation)
- **Embeddings** : BAAI/bge-m3 GPU (1024-dim, RTX 4070, PyTorch 2.6.0+cu124)
- **Handlers** : `memory/mcp/` (thought_tools, message_tools, conversation_tools)
- **DonnÃ©es** : 102 Thoughts, 377 Messages, 12 Conversations (au 2025-01-08)

#### Thought Tools (3)

**1. add_thought** - Ajouter une pensÃ©e au systÃ¨me
```
add_thought(
    content="Exploring vector databases for semantic search",
    thought_type="observation",  # reflection, question, intuition, observation
    trigger="Research session",
    concepts=["weaviate", "embeddings", "gpu"],
    privacy_level="private"  # private, shared, public
)
```

**2. search_thoughts** - Recherche sÃ©mantique dans les pensÃ©es
```
search_thoughts(
    query="vector databases GPU",
    limit=10,
    thought_type_filter="observation"  # optionnel
)
```

**3. get_thought** - RÃ©cupÃ©rer une pensÃ©e par UUID
```
get_thought(uuid="730c1a8e-b09f-4889-bbe9-4867d0ee7f1a")
```

#### Message Tools (3)

**4. add_message** - Ajouter un message Ã  une conversation
```
add_message(
    content="Explain transformers in AI",
    role="user",  # user, assistant, system
    conversation_id="chat_2025_01_08",
    order_index=0
)
```

**5. get_messages** - RÃ©cupÃ©rer tous les messages d'une conversation
```
get_messages(
    conversation_id="chat_2025_01_08",
    limit=50
)
```

**6. search_messages** - Recherche sÃ©mantique dans les messages
```
search_messages(
    query="transformers AI",
    limit=10,
    conversation_id_filter="chat_2025_01_08"  # optionnel
)
```

#### Conversation Tools (3)

**7. get_conversation** - RÃ©cupÃ©rer une conversation par ID
```
get_conversation(conversation_id="ikario_derniere_pensee")
```

**8. search_conversations** - Recherche sÃ©mantique dans les conversations
```
search_conversations(
    query="philosophical discussion",
    limit=10,
    category_filter="philosophy"  # optionnel
)
```

**9. list_conversations** - Lister toutes les conversations
```
list_conversations(
    limit=20,
    category_filter="testing"  # optionnel
)
```

**Tests** : Tous les outils Memory ont Ã©tÃ© testÃ©s avec succÃ¨s (voir `test_memory_mcp_tools.py`)

**Documentation complÃ¨te** : Voir `memory/README_MCP_TOOLS.md` pour l'architecture dÃ©taillÃ©e, les schÃ©mas de donnÃ©es et les exemples d'utilisation.

Pour plus de dÃ©tails sur les outils Library RAG, voir la documentation complÃ¨te dans `.claude/CLAUDE.md`.

---

## ğŸ’° Gestion des CoÃ»ts

### CoÃ»ts OCR (Mistral API)

| Mode | CoÃ»t par page | Utilisation |
|------|---------------|-------------|
| **Standard** | ~0.001-0.003â‚¬ | Extraction texte + images |
| **Avec annotations** | ~0.009â‚¬ (3x) | + Annotations structurelles (meilleure TOC) |

**Optimisation:** Utilisez `skip_ocr=True` pour rÃ©utiliser le Markdown existant et Ã©viter les coÃ»ts OCR lors du retraitement.

### CoÃ»ts LLM

| Provider | CoÃ»t | Performance |
|----------|------|-------------|
| **Ollama** (local) | Gratuit | Plus lent (~30s/doc), nÃ©cessite GPU/CPU puissant |
| **Mistral API** | Variable | Rapide (~5s/doc), facturÃ© par token |

**Recommandation:**
- DÃ©veloppement/test : Ollama (gratuit)
- Production : Mistral API (rapide, scalable)

### Suivi des CoÃ»ts

Chaque traitement gÃ©nÃ¨re un fichier `<doc>_chunks.json` avec :

```json
{
  "cost_ocr": 0.12,
  "cost_llm": 0.03,
  "total_cost": 0.15,
  "pages": 40,
  "chunks": 127
}
```

---

## ğŸ”§ Configuration Docker

Le fichier `docker-compose.yml` configure :

### Weaviate 1.34.4
- **Ports:** 8080 (HTTP), 50051 (gRPC)
- **Modules:** `text2vec-transformers`
- **Persistence:** Volume `weaviate_data`
- **Authentification:** DÃ©sactivÃ©e (dev local)

### text2vec-transformers
- **ModÃ¨le:** `baai-bge-m3-onnx` (BAAI/bge-m3, version ONNX)
- **Dimensions:** 1024 (2.7x plus riche que MiniLM-L6)
- **Context Window:** 8192 tokens (16x plus long que MiniLM-L6)
- **Runtime:** ONNX CPU-optimized (AVX2)
- **Multilingue:** Support supÃ©rieur pour grec, latin, franÃ§ais, anglais
- **Worker Timeout:** 600s (pour gÃ©rer les chunks trÃ¨s longs)

**Note GPU:** La version ONNX de BGE-M3 est CPU-only (pas de support CUDA natif dans ONNX runtime). Pour l'accÃ©lÃ©ration GPU, il faudrait utiliser NVIDIA NIM (architecture diffÃ©rente).

---

## ğŸ§ª Tests

```bash
# ExÃ©cuter tous les tests
pytest

# Tests spÃ©cifiques
pytest tests/utils/test_ocr_schemas.py -v

# Avec couverture
pytest --cov=utils --cov-report=html

# Type checking strict
mypy .
```

**Tests disponibles:**
- `test_ocr_schemas.py` : Validation schÃ©mas OCR
- `test_toc.py` : Extraction table des matiÃ¨res
- `test_mistral_client.py` : Client API Mistral

---

## ğŸ› Debugging

### ProblÃ¨mes Courants

**1. "Weaviate connection failed"**
```bash
# VÃ©rifier que les conteneurs sont dÃ©marrÃ©s
docker compose ps

# DÃ©marrer si nÃ©cessaire
docker compose up -d

# VÃ©rifier les logs
docker compose logs weaviate
```

**2. "OCR cost too high"**
```python
# RÃ©utiliser markdown existant
result = process_pdf(
    Path("input/document.pdf"),
    skip_ocr=True,  # â† Ã‰vite l'OCR
    use_llm=True,
)
```

**3. "LLM timeout (Ollama)"**
```env
# Augmenter timeout ou utiliser modÃ¨le plus lÃ©ger
STRUCTURE_LLM_MODEL=qwen2.5:7b  # Au lieu de deepseek-r1:14b
```

**4. "Empty chunks after cleaning"**
```python
# VÃ©rifier les sections classifiÃ©es
import json
with open("output/<doc>/<doc>_chunks.json") as f:
    data = json.load(f)
    print(data["classified_sections"])
```

**5. "TOC extraction failed"**
```python
# Utiliser annotations OCR (plus fiable mais 3x coÃ»t)
result = process_pdf(
    Path("input/document.pdf"),
    use_ocr_annotations=True,  # â† Meilleure TOC
)
```

**6. "Le fichier _ocr.json est-il utilisÃ© ?"**

Le fichier `<doc>_ocr.json` est crÃ©Ã© systÃ©matiquement mais :
- **Pipeline normal:** âŒ Non utilisÃ© (rÃ©ponse OCR en mÃ©moire â†’ markdown)
- **Mode `skip_ocr=True`:** âœ… Lu uniquement pour rÃ©cupÃ©rer le nombre de pages

**UtilitÃ©:** Archive en production, cache en dÃ©veloppement pour Ã©viter les coÃ»ts API.

### Logs

```python
import logging

# Activer logs dÃ©taillÃ©s
logging.basicConfig(level=logging.DEBUG)

# Logs pipeline
logger = logging.getLogger("utils.pdf_pipeline")
logger.setLevel(logging.DEBUG)
```

---

## ğŸš€ Production

### Checklist DÃ©ploiement

- [ ] **SÃ©curitÃ©:** Ajouter authentification Flask (Flask-Login, OAuth)
- [ ] **Rate Limiting:** Limiter uploads (Flask-Limiter)
- [ ] **Secrets:** Utiliser gestionnaire secrets (AWS Secrets Manager, Vault)
- [ ] **HTTPS:** Configurer reverse proxy (nginx + Let's Encrypt)
- [ ] **CORS:** Configurer CORS si API sÃ©parÃ©e
- [ ] **Monitoring:** Logging centralisÃ© (Sentry, CloudWatch)
- [ ] **CoÃ»ts:** Dashboard suivi coÃ»ts OCR/LLM
- [ ] **Backup:** StratÃ©gie backup Weaviate (volumes Docker)
- [ ] **Tests:** Suite tests complÃ¨te (pytest + couverture >80%)
- [ ] **CI/CD:** Pipeline automatisÃ© (GitHub Actions, GitLab CI)

### Exemple Nginx

```nginx
server {
    listen 80;
    server_name library-rag.example.com;

    location / {
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # SSE requiert des timeouts longs
    location /upload/progress {
        proxy_pass http://127.0.0.1:5000;
        proxy_buffering off;
        proxy_read_timeout 600s;
    }
}
```

### Production WSGI

```bash
# Installer Gunicorn
pip install gunicorn

# Lancer avec workers
gunicorn -w 4 -b 0.0.0.0:5000 --timeout 600 flask_app:app
```

---

## ğŸ“ Structure du Projet

```
library_rag/
â”œâ”€â”€ .env                        # Variables d'environnement (API keys, config LLM)
â”œâ”€â”€ .env.example                # Exemple de configuration
â”œâ”€â”€ docker-compose.yml          # Weaviate + text2vec-transformers
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ mypy.ini                    # Configuration mypy (strict mode)
â”œâ”€â”€ pytest.ini                  # Configuration pytest
â”‚
â”œâ”€â”€ schema.py                   # âš™ï¸ DÃ©finition schÃ©ma Weaviate RAG (3 collections: Work, Chunk_v2, Summary_v2)
â”œâ”€â”€ flask_app.py                # ğŸŒ Application Flask principale (38 Ko)
â”œâ”€â”€ mcp_server.py               # ğŸ¤– MCP server pour Claude Desktop
â”œâ”€â”€ query_test.py               # ğŸ” Exemples de requÃªtes sÃ©mantiques
â”‚
â”œâ”€â”€ utils/                      # ğŸ“¦ Modules du pipeline PDF
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ types.py                # TypedDict centralisÃ©es (31 Ko)
â”‚   â”œâ”€â”€ pdf_pipeline.py         # Orchestration pipeline 10 Ã©tapes (64 Ko)
â”‚   â”œâ”€â”€ mistral_client.py       # Client API Mistral OCR
â”‚   â”œâ”€â”€ pdf_uploader.py         # Upload PDF vers Mistral
â”‚   â”œâ”€â”€ ocr_processor.py        # Traitement OCR
â”‚   â”œâ”€â”€ ocr_schemas.py          # Types pour rÃ©ponses OCR
â”‚   â”œâ”€â”€ markdown_builder.py     # Construction Markdown
â”‚   â”œâ”€â”€ image_extractor.py      # Extraction images
â”‚   â”œâ”€â”€ hierarchy_parser.py     # Parsing hiÃ©rarchique
â”‚   â”œâ”€â”€ llm_structurer.py       # Infrastructure LLM (Ollama/Mistral)
â”‚   â”œâ”€â”€ llm_metadata.py         # LLM: Extraction mÃ©tadonnÃ©es
â”‚   â”œâ”€â”€ llm_toc.py              # LLM: Extraction TOC
â”‚   â”œâ”€â”€ llm_classifier.py       # LLM: Classification sections
â”‚   â”œâ”€â”€ llm_chunker.py          # LLM: Chunking sÃ©mantique
â”‚   â”œâ”€â”€ llm_cleaner.py          # Nettoyage chunks
â”‚   â”œâ”€â”€ llm_validator.py        # LLM: Validation + concepts
â”‚   â”œâ”€â”€ llm_summarizer.py       # LLM: GÃ©nÃ©ration rÃ©sumÃ©s chunks (optionnel)
â”‚   â”œâ”€â”€ weaviate_ingest.py      # Ingestion batch Weaviate
â”‚   â”œâ”€â”€ generate_chunk_summaries.py  # Script gÃ©nÃ©ration rÃ©sumÃ©s par batch
â”‚   â”œâ”€â”€ generate_all_summaries.py    # Script gÃ©nÃ©ration pour tous les docs
â”‚   â”œâ”€â”€ toc_extractor.py        # Extraction TOC (stratÃ©gies alternatives)
â”‚   â”œâ”€â”€ toc_extractor_markdown.py
â”‚   â””â”€â”€ toc_extractor_visual.py
â”‚
â”œâ”€â”€ mcp_tools/                  # ğŸ”§ MCP tool implementations (RAG)
â”‚   â”œâ”€â”€ parse_pdf.py
â”‚   â””â”€â”€ search.py
â”‚
â”œâ”€â”€ memory/                     # ğŸ§  Module Memory (3 collections sÃ©parÃ©es)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # GPU embedder (BAAI/bge-m3, RTX 4070)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ memory_schemas.py     # SchÃ©mas Conversation, Message, Thought
â”‚   â””â”€â”€ mcp/
â”‚       â”œâ”€â”€ conversation_tools.py # Outils MCP conversations
â”‚       â”œâ”€â”€ message_tools.py      # Outils MCP messages
â”‚       â””â”€â”€ thought_tools.py      # Outils MCP thoughts
â”‚
â”œâ”€â”€ templates/                  # ğŸ¨ Templates Jinja2
â”‚   â”œâ”€â”€ base.html               # Template de base (navigation, CSS)
â”‚   â”œâ”€â”€ index.html              # Page d'accueil (statistiques)
â”‚   â”œâ”€â”€ passages.html           # Liste paginÃ©e des chunks
â”‚   â”œâ”€â”€ search.html             # Interface de recherche sÃ©mantique
â”‚   â”œâ”€â”€ upload.html             # Formulaire d'upload PDF
â”‚   â”œâ”€â”€ upload_progress.html    # Progression SSE en temps rÃ©el
â”‚   â”œâ”€â”€ upload_result.html      # RÃ©sultats du traitement
â”‚   â”œâ”€â”€ documents.html          # Liste des documents traitÃ©s
â”‚   â””â”€â”€ document_view.html      # Vue dÃ©taillÃ©e d'un document
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ rag-philo-charte.css    # ğŸ¨ Charte graphique
â”‚
â”œâ”€â”€ input/                      # ğŸ“„ PDFs Ã  traiter
â”‚   â””â”€â”€ (vos fichiers PDF)
â”‚
â”œâ”€â”€ output/                     # ğŸ’¾ RÃ©sultats du traitement
â”‚   â””â”€â”€ <nom_document>/
â”‚       â”œâ”€â”€ <nom_document>.md             # Markdown structurÃ©
â”‚       â”œâ”€â”€ <nom_document>_chunks.json    # Chunks + mÃ©tadonnÃ©es
â”‚       â”œâ”€â”€ <nom_document>_ocr.json       # RÃ©ponse OCR brute
â”‚       â”œâ”€â”€ <nom_document>_weaviate.json  # RÃ©sultat ingestion
â”‚       â””â”€â”€ images/                       # Images extraites
â”‚           â”œâ”€â”€ page_001_image_0.png
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/                      # ğŸ§ª Tests unitaires
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ test_ocr_schemas.py
â”‚       â”œâ”€â”€ test_toc.py
â”‚       â””â”€â”€ test_mistral_client.py
â”‚
â”œâ”€â”€ .claude/                    # ğŸ¤– Instructions pour Claude Code
â”‚   â””â”€â”€ CLAUDE.md
â”‚
â””â”€â”€ README.md                   # ğŸ“– Ce fichier
```

---

## ğŸ“š Ressources

### Documentation

- [Weaviate Documentation](https://weaviate.io/developers/weaviate)
- [Weaviate Python Client v4](https://weaviate.io/developers/weaviate/client-libraries/python)
- [text2vec-transformers](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers)
- [Mistral AI API](https://docs.mistral.ai/)
- [Ollama Documentation](https://ollama.ai/)
- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)

### DÃ©veloppement

- `.claude/CLAUDE.md` - Instructions dÃ©veloppement pour Claude Code
- `utils/types.py` - DÃ©finitions TypedDict centralisÃ©es (31 Ko)
- `mypy.ini` - Configuration vÃ©rification types stricte

### ModÃ¨les

- **BAAI/bge-m3:** ModÃ¨le d'embedding multilingue (1024 dimensions, 8192 token context)
- **Qwen 2.5:** ModÃ¨le LLM recommandÃ© pour extraction (via Ollama)
- **Mistral API:** OCR + LLM cloud (rapide, payant)

---

## ğŸ“ Licence

Ce projet est un outil de recherche acadÃ©mique. Consultez votre licence spÃ©cifique.

---

## ğŸ¤ Contribution

Pour contribuer :

1. **Type Safety:** Toutes les fonctions doivent avoir des annotations de types
2. **Docstrings:** Google-style docstrings obligatoires
3. **Tests:** Ajouter tests unitaires pour nouvelles fonctionnalitÃ©s
4. **mypy:** Code doit passer `mypy --strict`
5. **SimplicitÃ©:** Suivre principes KISS et YAGNI

```bash
# VÃ©rifier types
mypy .

# VÃ©rifier docstrings
pydocstyle utils/

# Tests
pytest
```

---

## ğŸ“Œ Annexes

### Annexe: Migration BGE-M3

**Date:** DÃ©cembre 2024

**Raison:** Migration de MiniLM-L6 (384-dim) vers BAAI/bge-m3 (1024-dim) pour :
- 2.7Ã— reprÃ©sentation sÃ©mantique plus riche
- 8192 token context (vs 512)
- Support multilingue supÃ©rieur (grec, latin, franÃ§ais, anglais)
- Meilleures performances sur textes philosophiques/acadÃ©miques

**Impact:**
- **Aucun changement** dans le pipeline (Ã©tapes 1-9)
- **Modification** de la vectorisation (Ã©tape 10) : utilise BGE-M3
- **Collections Weaviate** : RecrÃ©Ã©es avec vecteurs 1024-dim
- **Documents existants** : Doivent Ãªtre rÃ©-ingÃ©rÃ©s

**Migration:**
```bash
# 1. ArrÃªter containers
docker compose down

# 2. DÃ©marrer avec nouvelle config
docker compose up -d

# 3. RecrÃ©er schÃ©ma
python schema.py

# 4. RÃ©-ingÃ©rer documents depuis cache
python reingest_from_cache.py
```

**Rollback:** Restaurer `docker-compose.yml.backup` si nÃ©cessaire (~15 min).

**Note Technique:** La version ONNX de BGE-M3 est CPU-only (pas de VRAM utilisÃ©e). Pour l'accÃ©lÃ©ration GPU, il faudrait utiliser NVIDIA NIM (architecture diffÃ©rente).

---

**Library RAG** - SystÃ¨me RAG de qualitÃ© production pour textes philosophiques et acadÃ©miques.
